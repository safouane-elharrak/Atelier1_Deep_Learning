{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e74407",
   "metadata": {},
   "source": [
    "# Structured data learning with Wide, Deep, and Cross networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4024b4",
   "metadata": {},
   "source": [
    "#### This illustration shows how to classify structured data using the two modeling techniques: models Wide & Deep models Deep & Cross\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1493f320",
   "metadata": {},
   "source": [
    "#### The Covertype dataset from the UCI Machine Learning Repository is used in this example. Predicting the kind of forest cover from geographic factors is the task. There are 506,011 cases in the dataset with 12 input features—10 numerical and 2 categorical. Each instance is put into one of seven classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3192e63",
   "metadata": {},
   "source": [
    "### import all libraries that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6abfd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15205434",
   "metadata": {},
   "source": [
    "### Download the data from the url and load it into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec27af66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (581012, 55)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2596</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "      <td>221</td>\n",
       "      <td>232</td>\n",
       "      <td>148</td>\n",
       "      <td>6279</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>6225</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2595</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>-1</td>\n",
       "      <td>391</td>\n",
       "      <td>220</td>\n",
       "      <td>234</td>\n",
       "      <td>150</td>\n",
       "      <td>6172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1   2    3    4     5    6    7    8     9   ...  45  46  47  48  \\\n",
       "0  2596   51   3  258    0   510  221  232  148  6279  ...   0   0   0   0   \n",
       "1  2590   56   2  212   -6   390  220  235  151  6225  ...   0   0   0   0   \n",
       "2  2804  139   9  268   65  3180  234  238  135  6121  ...   0   0   0   0   \n",
       "3  2785  155  18  242  118  3090  238  238  122  6211  ...   0   0   0   0   \n",
       "4  2595   45   2  153   -1   391  220  234  150  6172  ...   0   0   0   0   \n",
       "\n",
       "   49  50  51  52  53  54  \n",
       "0   0   0   0   0   0   5  \n",
       "1   0   0   0   0   0   5  \n",
       "2   0   0   0   0   0   2  \n",
       "3   0   0   0   0   0   2  \n",
       "4   0   0   0   0   0   5  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_url = (\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\"\n",
    ")\n",
    "raw_data = pd.read_csv(data_url, header=None)\n",
    "print(f\"Dataset shape: {raw_data.shape}\")\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf27f6b",
   "metadata": {},
   "source": [
    "### The dataset contains two binary-encoded category features. We'll change the representation of this dataset to the standard representation, in which each categorical feature is represented by a single integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c01b217c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (581012, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Elevation</th>\n",
       "      <td>2596</td>\n",
       "      <td>2590</td>\n",
       "      <td>2804</td>\n",
       "      <td>2785</td>\n",
       "      <td>2595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aspect</th>\n",
       "      <td>51</td>\n",
       "      <td>56</td>\n",
       "      <td>139</td>\n",
       "      <td>155</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Slope</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <td>258</td>\n",
       "      <td>212</td>\n",
       "      <td>268</td>\n",
       "      <td>242</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <td>0</td>\n",
       "      <td>-6</td>\n",
       "      <td>65</td>\n",
       "      <td>118</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <td>510</td>\n",
       "      <td>390</td>\n",
       "      <td>3180</td>\n",
       "      <td>3090</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <td>221</td>\n",
       "      <td>220</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <td>232</td>\n",
       "      <td>235</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <td>148</td>\n",
       "      <td>151</td>\n",
       "      <td>135</td>\n",
       "      <td>122</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <td>6279</td>\n",
       "      <td>6225</td>\n",
       "      <td>6121</td>\n",
       "      <td>6211</td>\n",
       "      <td>6172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area</th>\n",
       "      <td>area_type_1</td>\n",
       "      <td>area_type_1</td>\n",
       "      <td>area_type_1</td>\n",
       "      <td>area_type_1</td>\n",
       "      <td>area_type_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type</th>\n",
       "      <td>soil_type_29</td>\n",
       "      <td>soil_type_29</td>\n",
       "      <td>soil_type_12</td>\n",
       "      <td>soil_type_30</td>\n",
       "      <td>soil_type_29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cover_Type</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               0             1             2  \\\n",
       "Elevation                                   2596          2590          2804   \n",
       "Aspect                                        51            56           139   \n",
       "Slope                                          3             2             9   \n",
       "Horizontal_Distance_To_Hydrology             258           212           268   \n",
       "Vertical_Distance_To_Hydrology                 0            -6            65   \n",
       "Horizontal_Distance_To_Roadways              510           390          3180   \n",
       "Hillshade_9am                                221           220           234   \n",
       "Hillshade_Noon                               232           235           238   \n",
       "Hillshade_3pm                                148           151           135   \n",
       "Horizontal_Distance_To_Fire_Points          6279          6225          6121   \n",
       "Wilderness_Area                      area_type_1   area_type_1   area_type_1   \n",
       "Soil_Type                           soil_type_29  soil_type_29  soil_type_12   \n",
       "Cover_Type                                     4             4             1   \n",
       "\n",
       "                                               3             4  \n",
       "Elevation                                   2785          2595  \n",
       "Aspect                                       155            45  \n",
       "Slope                                         18             2  \n",
       "Horizontal_Distance_To_Hydrology             242           153  \n",
       "Vertical_Distance_To_Hydrology               118            -1  \n",
       "Horizontal_Distance_To_Roadways             3090           391  \n",
       "Hillshade_9am                                238           220  \n",
       "Hillshade_Noon                               238           234  \n",
       "Hillshade_3pm                                122           150  \n",
       "Horizontal_Distance_To_Fire_Points          6211          6172  \n",
       "Wilderness_Area                      area_type_1   area_type_1  \n",
       "Soil_Type                           soil_type_30  soil_type_29  \n",
       "Cover_Type                                     1             4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soil_type_values = [f\"soil_type_{idx+1}\" for idx in range(40)]\n",
    "wilderness_area_values = [f\"area_type_{idx+1}\" for idx in range(4)]\n",
    "\n",
    "soil_type = raw_data.loc[:, 14:53].apply(\n",
    "    lambda x: soil_type_values[0::1][x.to_numpy().nonzero()[0][0]], axis=1\n",
    ")\n",
    "wilderness_area = raw_data.loc[:, 10:13].apply(\n",
    "    lambda x: wilderness_area_values[0::1][x.to_numpy().nonzero()[0][0]], axis=1\n",
    ")\n",
    "\n",
    "CSV_HEADER = [\n",
    "    \"Elevation\",\n",
    "    \"Aspect\",\n",
    "    \"Slope\",\n",
    "    \"Horizontal_Distance_To_Hydrology\",\n",
    "    \"Vertical_Distance_To_Hydrology\",\n",
    "    \"Horizontal_Distance_To_Roadways\",\n",
    "    \"Hillshade_9am\",\n",
    "    \"Hillshade_Noon\",\n",
    "    \"Hillshade_3pm\",\n",
    "    \"Horizontal_Distance_To_Fire_Points\",\n",
    "    \"Wilderness_Area\",\n",
    "    \"Soil_Type\",\n",
    "    \"Cover_Type\",\n",
    "]\n",
    "\n",
    "data = pd.concat(\n",
    "    [raw_data.loc[:, 0:9], wilderness_area, soil_type, raw_data.loc[:, 54]],\n",
    "    axis=1,\n",
    "    ignore_index=True,\n",
    ")\n",
    "data.columns = CSV_HEADER\n",
    "\n",
    "# Convert the target label indices into a range from 0 to 6 (there are 7 labels in total).\n",
    "data[\"Cover_Type\"] = data[\"Cover_Type\"] - 1\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "data.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f810cace",
   "metadata": {},
   "source": [
    "### Split the data into training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5635f108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split size: 494237\n",
      "Test split size: 86775\n"
     ]
    }
   ],
   "source": [
    "train_splits = []\n",
    "test_splits = []\n",
    "\n",
    "for _, group_data in data.groupby(\"Cover_Type\"):\n",
    "    random_selection = np.random.rand(len(group_data.index)) <= 0.85\n",
    "    train_splits.append(group_data[random_selection])\n",
    "    test_splits.append(group_data[~random_selection])\n",
    "\n",
    "train_data = pd.concat(train_splits).sample(frac=1).reset_index(drop=True)\n",
    "test_data = pd.concat(test_splits).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train split size: {len(train_data.index)}\")\n",
    "print(f\"Test split size: {len(test_data.index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e5b502",
   "metadata": {},
   "source": [
    "### the training and test data should then be kept in separate CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc90fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file = \"train_data.csv\"\n",
    "test_data_file = \"test_data.csv\"\n",
    "\n",
    "train_data.to_csv(train_data_file, index=False)\n",
    "test_data.to_csv(test_data_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a3b648",
   "metadata": {},
   "source": [
    "### we define thedataset's metadata, which will be used for reading, parsing, and encoding the input features according to their kinds after the data has been divided into input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c9e7f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_FEATURE_NAME = \"Cover_Type\"\n",
    "\n",
    "TARGET_FEATURE_LABELS = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n",
    "\n",
    "NUMERIC_FEATURE_NAMES = [\n",
    "    \"Aspect\",\n",
    "    \"Elevation\",\n",
    "    \"Hillshade_3pm\",\n",
    "    \"Hillshade_9am\",\n",
    "    \"Hillshade_Noon\",\n",
    "    \"Horizontal_Distance_To_Fire_Points\",\n",
    "    \"Horizontal_Distance_To_Hydrology\",\n",
    "    \"Horizontal_Distance_To_Roadways\",\n",
    "    \"Slope\",\n",
    "    \"Vertical_Distance_To_Hydrology\",\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"Soil_Type\": list(data[\"Soil_Type\"].unique()),\n",
    "    \"Wilderness_Area\": list(data[\"Wilderness_Area\"].unique()),\n",
    "}\n",
    "\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
    "\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "\n",
    "COLUMN_DEFAULTS = [\n",
    "    [0] if feature_name in NUMERIC_FEATURE_NAMES + [TARGET_FEATURE_NAME] else [\"NA\"]\n",
    "    for feature_name in CSV_HEADER\n",
    "]\n",
    "\n",
    "NUM_CLASSES = len(TARGET_FEATURE_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cd59c0",
   "metadata": {},
   "source": [
    "### define an input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9699913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_from_csv(csv_file_path, batch_size, shuffle=False):\n",
    "\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_HEADER,\n",
    "        column_defaults=COLUMN_DEFAULTS,\n",
    "        label_name=TARGET_FEATURE_NAME,\n",
    "        num_epochs=1,\n",
    "        header=True,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return dataset.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a2e5ea",
   "metadata": {},
   "source": [
    "### Using a model as a starting point, configure the parameters and carry out the procedure to perform the training and evaluation experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5023f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "dropout_rate = 0.1\n",
    "batch_size = 265\n",
    "num_epochs = 50\n",
    "\n",
    "hidden_units = [32, 32]\n",
    "\n",
    "\n",
    "def run_experiment(model):\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "    train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = get_dataset_from_csv(test_data_file, batch_size)\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "    history = model.fit(train_dataset, epochs=num_epochs)\n",
    "    print(\"Model training finished\")\n",
    "\n",
    "    _, accuracy = model.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c269c9f3",
   "metadata": {},
   "source": [
    "### Create model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdf196ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.float32\n",
    "            )\n",
    "        else:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.string\n",
    "            )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88997dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import StringLookup\n",
    "\n",
    "\n",
    "def encode_inputs(inputs, use_embedding=False):\n",
    "    encoded_features = []\n",
    "    for feature_name in inputs:\n",
    "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "            # Create a lookup to convert string values to an integer indices.\n",
    "            # Since we are not using a mask token nor expecting any out of vocabulary\n",
    "            # (oov) token, we set mask_token to None and  num_oov_indices to 0.\n",
    "            lookup = StringLookup(\n",
    "                vocabulary=vocabulary,\n",
    "                mask_token=None,\n",
    "                num_oov_indices=0,\n",
    "                output_mode=\"int\" if use_embedding else \"binary\",\n",
    "            )\n",
    "            if use_embedding:\n",
    "                # Convert the string input values into integer indices.\n",
    "                encoded_feature = lookup(inputs[feature_name])\n",
    "                embedding_dims = int(math.sqrt(len(vocabulary)))\n",
    "                # Create an embedding layer with the specified dimensions.\n",
    "                embedding = layers.Embedding(\n",
    "                    input_dim=len(vocabulary), output_dim=embedding_dims\n",
    "                )\n",
    "                # Convert the index values to embedding representations.\n",
    "                encoded_feature = embedding(encoded_feature)\n",
    "            else:\n",
    "                # Convert the string input values into a one hot encoding.\n",
    "                encoded_feature = lookup(tf.expand_dims(inputs[feature_name], -1))\n",
    "        else:\n",
    "            # Use the numerical features as-is.\n",
    "            encoded_feature = tf.expand_dims(inputs[feature_name], -1)\n",
    "\n",
    "        encoded_features.append(encoded_feature)\n",
    "\n",
    "    all_features = layers.concatenate(encoded_features)\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b040d7",
   "metadata": {},
   "source": [
    "### Let's construct a multi-layer feed-forward network for the first experiment, with the categorical features one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95b39cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Safouane Elh\\anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py:2446: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "def create_baseline_model():\n",
    "    inputs = create_model_inputs()\n",
    "    features = encode_inputs(inputs)\n",
    "\n",
    "    for units in hidden_units:\n",
    "        features = layers.Dense(units)(features)\n",
    "        features = layers.BatchNormalization()(features)\n",
    "        features = layers.ReLU()(features)\n",
    "        features = layers.Dropout(dropout_rate)(features)\n",
    "\n",
    "    outputs = layers.Dense(units=NUM_CLASSES, activation=\"softmax\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "baseline_model = create_baseline_model()\n",
    "keras.utils.plot_model(baseline_model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "decf942c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/50\n",
      "1866/1866 [==============================] - 32s 14ms/step - loss: 0.7834 - sparse_categorical_accuracy: 0.6781\n",
      "Epoch 2/50\n",
      "1866/1866 [==============================] - 21s 11ms/step - loss: 0.6703 - sparse_categorical_accuracy: 0.7121\n",
      "Epoch 3/50\n",
      "1866/1866 [==============================] - 20s 11ms/step - loss: 0.6444 - sparse_categorical_accuracy: 0.7222\n",
      "Epoch 4/50\n",
      "1866/1866 [==============================] - 19s 10ms/step - loss: 0.6252 - sparse_categorical_accuracy: 0.7300\n",
      "Epoch 5/50\n",
      "1866/1866 [==============================] - 17s 9ms/step - loss: 0.6119 - sparse_categorical_accuracy: 0.7359\n",
      "Epoch 6/50\n",
      "1866/1866 [==============================] - 18s 10ms/step - loss: 0.6022 - sparse_categorical_accuracy: 0.7409\n",
      "Epoch 7/50\n",
      "1866/1866 [==============================] - 18s 9ms/step - loss: 0.5940 - sparse_categorical_accuracy: 0.7441\n",
      "Epoch 8/50\n",
      "1866/1866 [==============================] - 20s 11ms/step - loss: 0.5893 - sparse_categorical_accuracy: 0.7468\n",
      "Epoch 9/50\n",
      "1866/1866 [==============================] - 21s 12ms/step - loss: 0.5836 - sparse_categorical_accuracy: 0.7483\n",
      "Epoch 10/50\n",
      "1866/1866 [==============================] - 18s 10ms/step - loss: 0.5810 - sparse_categorical_accuracy: 0.7499\n",
      "Epoch 11/50\n",
      "1866/1866 [==============================] - 18s 10ms/step - loss: 0.5770 - sparse_categorical_accuracy: 0.7509\n",
      "Epoch 12/50\n",
      "1866/1866 [==============================] - 23s 12ms/step - loss: 0.5741 - sparse_categorical_accuracy: 0.7521\n",
      "Epoch 13/50\n",
      "1866/1866 [==============================] - 22s 12ms/step - loss: 0.5724 - sparse_categorical_accuracy: 0.7528\n",
      "Epoch 14/50\n",
      "1866/1866 [==============================] - 21s 11ms/step - loss: 0.5690 - sparse_categorical_accuracy: 0.7546\n",
      "Epoch 15/50\n",
      "1866/1866 [==============================] - 22s 12ms/step - loss: 0.5673 - sparse_categorical_accuracy: 0.7555\n",
      "Epoch 16/50\n",
      "1866/1866 [==============================] - 17s 9ms/step - loss: 0.5656 - sparse_categorical_accuracy: 0.7560\n",
      "Epoch 17/50\n",
      "1866/1866 [==============================] - 19s 10ms/step - loss: 0.5635 - sparse_categorical_accuracy: 0.7575\n",
      "Epoch 18/50\n",
      "1866/1866 [==============================] - 21s 11ms/step - loss: 0.5611 - sparse_categorical_accuracy: 0.7581\n",
      "Epoch 19/50\n",
      "1866/1866 [==============================] - 23s 12ms/step - loss: 0.5597 - sparse_categorical_accuracy: 0.7580\n",
      "Epoch 20/50\n",
      "1866/1866 [==============================] - 19s 10ms/step - loss: 0.5577 - sparse_categorical_accuracy: 0.7593\n",
      "Epoch 21/50\n",
      "1866/1866 [==============================] - 18s 10ms/step - loss: 0.5573 - sparse_categorical_accuracy: 0.7599\n",
      "Epoch 22/50\n",
      "1866/1866 [==============================] - 18s 10ms/step - loss: 0.5551 - sparse_categorical_accuracy: 0.7604\n",
      "Epoch 23/50\n",
      "1866/1866 [==============================] - 19s 10ms/step - loss: 0.5543 - sparse_categorical_accuracy: 0.7606\n",
      "Epoch 24/50\n",
      "1866/1866 [==============================] - 19s 10ms/step - loss: 0.5533 - sparse_categorical_accuracy: 0.7609\n",
      "Epoch 25/50\n",
      "1866/1866 [==============================] - 19s 10ms/step - loss: 0.5519 - sparse_categorical_accuracy: 0.7612\n",
      "Epoch 26/50\n",
      "1866/1866 [==============================] - 20s 11ms/step - loss: 0.5511 - sparse_categorical_accuracy: 0.7617\n",
      "Epoch 27/50\n",
      "1866/1866 [==============================] - 18s 10ms/step - loss: 0.5504 - sparse_categorical_accuracy: 0.7621\n",
      "Epoch 28/50\n",
      "1866/1866 [==============================] - 21s 11ms/step - loss: 0.5500 - sparse_categorical_accuracy: 0.7628\n",
      "Epoch 29/50\n",
      "1866/1866 [==============================] - 20s 11ms/step - loss: 0.5504 - sparse_categorical_accuracy: 0.7623\n",
      "Epoch 30/50\n",
      "1866/1866 [==============================] - 20s 11ms/step - loss: 0.5470 - sparse_categorical_accuracy: 0.7641\n",
      "Epoch 31/50\n",
      "1866/1866 [==============================] - 21s 11ms/step - loss: 0.5463 - sparse_categorical_accuracy: 0.7635\n",
      "Epoch 32/50\n",
      "1866/1866 [==============================] - 18s 10ms/step - loss: 0.5459 - sparse_categorical_accuracy: 0.7647\n",
      "Epoch 33/50\n",
      "1866/1866 [==============================] - 18s 9ms/step - loss: 0.5448 - sparse_categorical_accuracy: 0.7649\n",
      "Epoch 34/50\n",
      "1866/1866 [==============================] - 18s 9ms/step - loss: 0.5442 - sparse_categorical_accuracy: 0.7646\n",
      "Epoch 35/50\n",
      "1866/1866 [==============================] - 18s 10ms/step - loss: 0.5431 - sparse_categorical_accuracy: 0.7653\n",
      "Epoch 36/50\n",
      "1866/1866 [==============================] - 18s 10ms/step - loss: 0.5432 - sparse_categorical_accuracy: 0.7654\n",
      "Epoch 37/50\n",
      "1866/1866 [==============================] - 18s 10ms/step - loss: 0.5428 - sparse_categorical_accuracy: 0.7659\n",
      "Epoch 38/50\n",
      "1866/1866 [==============================] - 19s 10ms/step - loss: 0.5415 - sparse_categorical_accuracy: 0.7664\n",
      "Epoch 39/50\n",
      "1866/1866 [==============================] - 20s 11ms/step - loss: 0.5411 - sparse_categorical_accuracy: 0.7667\n",
      "Epoch 40/50\n",
      "1866/1866 [==============================] - 19s 10ms/step - loss: 0.5407 - sparse_categorical_accuracy: 0.7660\n",
      "Epoch 41/50\n",
      "1866/1866 [==============================] - 19s 10ms/step - loss: 0.5393 - sparse_categorical_accuracy: 0.7675\n",
      "Epoch 42/50\n",
      "1866/1866 [==============================] - 25s 13ms/step - loss: 0.5389 - sparse_categorical_accuracy: 0.7676\n",
      "Epoch 43/50\n",
      "1866/1866 [==============================] - 27s 15ms/step - loss: 0.5379 - sparse_categorical_accuracy: 0.7677\n",
      "Epoch 44/50\n",
      "1866/1866 [==============================] - 27s 15ms/step - loss: 0.5376 - sparse_categorical_accuracy: 0.7680\n",
      "Epoch 45/50\n",
      "1866/1866 [==============================] - 27s 14ms/step - loss: 0.5384 - sparse_categorical_accuracy: 0.7674\n",
      "Epoch 46/50\n",
      "1866/1866 [==============================] - 26s 14ms/step - loss: 0.5376 - sparse_categorical_accuracy: 0.7676\n",
      "Epoch 47/50\n",
      "1866/1866 [==============================] - 27s 15ms/step - loss: 0.5362 - sparse_categorical_accuracy: 0.7682\n",
      "Epoch 48/50\n",
      "1866/1866 [==============================] - 27s 15ms/step - loss: 0.5370 - sparse_categorical_accuracy: 0.7681\n",
      "Epoch 49/50\n",
      "1866/1866 [==============================] - 23s 12ms/step - loss: 0.5359 - sparse_categorical_accuracy: 0.7679\n",
      "Epoch 50/50\n",
      "1866/1866 [==============================] - 19s 10ms/step - loss: 0.5353 - sparse_categorical_accuracy: 0.7690\n",
      "Model training finished\n",
      "Test accuracy: 75.68%\n"
     ]
    }
   ],
   "source": [
    "run_experiment(baseline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8644c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "def create_wide_and_deep_model():\n",
    "\n",
    "    inputs = create_model_inputs()\n",
    "    wide = encode_inputs(inputs)\n",
    "    wide = layers.BatchNormalization()(wide)\n",
    "\n",
    "    deep = encode_inputs(inputs, use_embedding=True)\n",
    "    for units in hidden_units:\n",
    "        deep = layers.Dense(units)(deep)\n",
    "        deep = layers.BatchNormalization()(deep)\n",
    "        deep = layers.ReLU()(deep)\n",
    "        deep = layers.Dropout(dropout_rate)(deep)\n",
    "\n",
    "    merged = layers.concatenate([wide, deep])\n",
    "    outputs = layers.Dense(units=NUM_CLASSES, activation=\"softmax\")(merged)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "wide_and_deep_model = create_wide_and_deep_model()\n",
    "keras.utils.plot_model(wide_and_deep_model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d1d5155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/50\n",
      "1866/1866 [==============================] - 29s 13ms/step - loss: 0.7127 - sparse_categorical_accuracy: 0.7036\n",
      "Epoch 2/50\n",
      "1866/1866 [==============================] - 20s 11ms/step - loss: 0.6057 - sparse_categorical_accuracy: 0.7374\n",
      "Epoch 3/50\n",
      "1866/1866 [==============================] - 22s 12ms/step - loss: 0.5880 - sparse_categorical_accuracy: 0.7442\n",
      "Epoch 4/50\n",
      "1866/1866 [==============================] - 23s 12ms/step - loss: 0.5743 - sparse_categorical_accuracy: 0.7510\n",
      "Epoch 5/50\n",
      "1866/1866 [==============================] - 29s 15ms/step - loss: 0.5668 - sparse_categorical_accuracy: 0.7539\n",
      "Epoch 6/50\n",
      "1866/1866 [==============================] - 28s 15ms/step - loss: 0.5597 - sparse_categorical_accuracy: 0.7573\n",
      "Epoch 7/50\n",
      "1866/1866 [==============================] - 28s 15ms/step - loss: 0.5542 - sparse_categorical_accuracy: 0.7604\n",
      "Epoch 8/50\n",
      "1866/1866 [==============================] - 26s 14ms/step - loss: 0.5486 - sparse_categorical_accuracy: 0.7627\n",
      "Epoch 9/50\n",
      "1866/1866 [==============================] - 28s 15ms/step - loss: 0.5447 - sparse_categorical_accuracy: 0.7655\n",
      "Epoch 10/50\n",
      "1866/1866 [==============================] - 25s 14ms/step - loss: 0.5411 - sparse_categorical_accuracy: 0.7668\n",
      "Epoch 11/50\n",
      "1866/1866 [==============================] - 22s 12ms/step - loss: 0.5371 - sparse_categorical_accuracy: 0.7688\n",
      "Epoch 12/50\n",
      "1866/1866 [==============================] - 25s 13ms/step - loss: 0.5341 - sparse_categorical_accuracy: 0.7699\n",
      "Epoch 13/50\n",
      "1866/1866 [==============================] - 19s 10ms/step - loss: 0.5315 - sparse_categorical_accuracy: 0.7711\n",
      "Epoch 14/50\n",
      "1866/1866 [==============================] - 18s 10ms/step - loss: 0.5291 - sparse_categorical_accuracy: 0.7724\n",
      "Epoch 15/50\n",
      "1866/1866 [==============================] - 19s 10ms/step - loss: 0.5266 - sparse_categorical_accuracy: 0.7737\n",
      "Epoch 16/50\n",
      "1866/1866 [==============================] - 19s 10ms/step - loss: 0.5248 - sparse_categorical_accuracy: 0.7750\n",
      "Epoch 17/50\n",
      "1866/1866 [==============================] - 19s 10ms/step - loss: 0.5230 - sparse_categorical_accuracy: 0.7761\n",
      "Epoch 18/50\n",
      "1866/1866 [==============================] - 19s 10ms/step - loss: 0.5208 - sparse_categorical_accuracy: 0.7767\n",
      "Epoch 19/50\n",
      "1866/1866 [==============================] - 19s 10ms/step - loss: 0.5193 - sparse_categorical_accuracy: 0.7780\n",
      "Epoch 20/50\n",
      "1866/1866 [==============================] - 20s 11ms/step - loss: 0.5183 - sparse_categorical_accuracy: 0.7788\n",
      "Epoch 21/50\n",
      "1866/1866 [==============================] - 20s 11ms/step - loss: 0.5172 - sparse_categorical_accuracy: 0.7793\n",
      "Epoch 22/50\n",
      "1866/1866 [==============================] - 28s 15ms/step - loss: 0.5164 - sparse_categorical_accuracy: 0.7796\n",
      "Epoch 23/50\n",
      "1866/1866 [==============================] - 22s 12ms/step - loss: 0.5157 - sparse_categorical_accuracy: 0.7804\n",
      "Epoch 24/50\n",
      "1866/1866 [==============================] - 19s 10ms/step - loss: 0.5142 - sparse_categorical_accuracy: 0.7814\n",
      "Epoch 25/50\n",
      "1866/1866 [==============================] - 15s 8ms/step - loss: 0.5140 - sparse_categorical_accuracy: 0.7811\n",
      "Epoch 26/50\n",
      "1866/1866 [==============================] - 22s 12ms/step - loss: 0.5127 - sparse_categorical_accuracy: 0.7814\n",
      "Epoch 27/50\n",
      "1866/1866 [==============================] - 22s 12ms/step - loss: 0.5116 - sparse_categorical_accuracy: 0.7820\n",
      "Epoch 28/50\n",
      "1866/1866 [==============================] - 19s 10ms/step - loss: 0.5113 - sparse_categorical_accuracy: 0.7824\n",
      "Epoch 29/50\n",
      "1866/1866 [==============================] - 19s 10ms/step - loss: 0.5109 - sparse_categorical_accuracy: 0.7832\n",
      "Epoch 30/50\n",
      "1866/1866 [==============================] - 23s 12ms/step - loss: 0.5093 - sparse_categorical_accuracy: 0.7832\n",
      "Epoch 31/50\n",
      "1866/1866 [==============================] - 20s 11ms/step - loss: 0.5096 - sparse_categorical_accuracy: 0.7836\n",
      "Epoch 32/50\n",
      "1866/1866 [==============================] - 18s 10ms/step - loss: 0.5093 - sparse_categorical_accuracy: 0.7835\n",
      "Epoch 33/50\n",
      "1866/1866 [==============================] - 19s 10ms/step - loss: 0.5077 - sparse_categorical_accuracy: 0.7847\n",
      "Epoch 34/50\n",
      "1866/1866 [==============================] - 18s 10ms/step - loss: 0.5078 - sparse_categorical_accuracy: 0.7842\n",
      "Epoch 35/50\n",
      "1866/1866 [==============================] - 23s 12ms/step - loss: 0.5063 - sparse_categorical_accuracy: 0.7846\n",
      "Epoch 36/50\n",
      "1866/1866 [==============================] - 22s 12ms/step - loss: 0.5055 - sparse_categorical_accuracy: 0.7847\n",
      "Epoch 37/50\n",
      "1866/1866 [==============================] - 21s 11ms/step - loss: 0.5058 - sparse_categorical_accuracy: 0.7851\n",
      "Epoch 38/50\n",
      "1866/1866 [==============================] - 22s 12ms/step - loss: 0.5049 - sparse_categorical_accuracy: 0.7855\n",
      "Epoch 39/50\n",
      "1866/1866 [==============================] - 20s 11ms/step - loss: 0.5048 - sparse_categorical_accuracy: 0.7853\n",
      "Epoch 40/50\n",
      "1866/1866 [==============================] - 19s 10ms/step - loss: 0.5038 - sparse_categorical_accuracy: 0.7863\n",
      "Epoch 41/50\n",
      "1866/1866 [==============================] - 20s 11ms/step - loss: 0.5032 - sparse_categorical_accuracy: 0.7863\n",
      "Epoch 42/50\n",
      "1866/1866 [==============================] - 24s 13ms/step - loss: 0.5032 - sparse_categorical_accuracy: 0.7864\n",
      "Epoch 43/50\n",
      "1866/1866 [==============================] - 28s 15ms/step - loss: 0.5022 - sparse_categorical_accuracy: 0.7868\n",
      "Epoch 44/50\n",
      "1866/1866 [==============================] - 28s 15ms/step - loss: 0.5028 - sparse_categorical_accuracy: 0.7867\n",
      "Epoch 45/50\n",
      "1866/1866 [==============================] - 21s 11ms/step - loss: 0.5027 - sparse_categorical_accuracy: 0.7860\n",
      "Epoch 46/50\n",
      "1866/1866 [==============================] - 21s 11ms/step - loss: 0.5016 - sparse_categorical_accuracy: 0.7868\n",
      "Epoch 47/50\n",
      "1866/1866 [==============================] - 16s 9ms/step - loss: 0.5020 - sparse_categorical_accuracy: 0.7868\n",
      "Epoch 48/50\n",
      "1866/1866 [==============================] - 22s 12ms/step - loss: 0.5000 - sparse_categorical_accuracy: 0.7873\n",
      "Epoch 49/50\n",
      "1866/1866 [==============================] - 18s 10ms/step - loss: 0.5004 - sparse_categorical_accuracy: 0.7874\n",
      "Epoch 50/50\n",
      "1866/1866 [==============================] - 28s 15ms/step - loss: 0.5000 - sparse_categorical_accuracy: 0.7877\n",
      "Model training finished\n",
      "Test accuracy: 81.06%\n"
     ]
    }
   ],
   "source": [
    "run_experiment(wide_and_deep_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c617841",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40093db",
   "metadata": {},
   "source": [
    "###  In this Lab i learn how we can manage categorical features with various encoding strategies, such as one-hot encoding and feature embedding, with ease by using Keras Preprocessing Layers. Additionally, various model topologies, including as wide, deep, and cross networks, offer various benefits in relation to certain dataset features. For the best outcome for your dataset, experiment with utilizing them separately or in combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076c18da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
