{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0786f2f",
   "metadata": {},
   "source": [
    "# Structured data learning with TabTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a119e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_addons\n",
      "  Downloading tensorflow_addons-0.18.0-cp39-cp39-win_amd64.whl (765 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\safouane elh\\anaconda3\\lib\\site-packages (from tensorflow_addons) (21.3)\n",
      "Collecting typeguard>=2.7\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\safouane elh\\anaconda3\\lib\\site-packages (from packaging->tensorflow_addons) (3.0.4)\n",
      "Installing collected packages: typeguard, tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.18.0 typeguard-2.13.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee451a08",
   "metadata": {},
   "source": [
    "##  Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00b1422",
   "metadata": {},
   "source": [
    "### Import all libraries that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91815d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ab651f",
   "metadata": {},
   "source": [
    "##  Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732c4532",
   "metadata": {},
   "source": [
    "### We load the dataset from the UCI Machine Learning Repository into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c0cea3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (32561, 15)\n",
      "Test dataset shape: (16282, 15)\n"
     ]
    }
   ],
   "source": [
    "CSV_HEADER = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education_num\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"gender\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "    \"native_country\",\n",
    "    \"income_bracket\",\n",
    "]\n",
    "\n",
    "train_data_url = (\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    ")\n",
    "train_data = pd.read_csv(train_data_url, header=None, names=CSV_HEADER)\n",
    "\n",
    "test_data_url = (\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
    ")\n",
    "test_data = pd.read_csv(test_data_url, header=None, names=CSV_HEADER)\n",
    "\n",
    "print(f\"Train dataset shape: {train_data.shape}\")\n",
    "print(f\"Test dataset shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f133338d",
   "metadata": {},
   "source": [
    "### Remove the first record (because it is not a valid data example) and a trailing 'dot' in the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e6d2390",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[1:]\n",
    "test_data.income_bracket = test_data.income_bracket.apply(\n",
    "    lambda value: value.replace(\".\", \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c962021",
   "metadata": {},
   "source": [
    "### We store the training and test data in separate CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba618c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file = \"train_data.csv\"\n",
    "test_data_file = \"test_data.csv\"\n",
    "\n",
    "train_data.to_csv(train_data_file, index=False, header=False)\n",
    "test_data.to_csv(test_data_file, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e657e",
   "metadata": {},
   "source": [
    "##  Define dataset metada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9a4554",
   "metadata": {},
   "source": [
    "###  We define the metadata of the dataset that will be useful for reading and parsing the data into input features, and encoding the input features with respect to their types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3182e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of the numerical feature names.\n",
    "NUMERIC_FEATURE_NAMES = [\n",
    "    \"age\",\n",
    "    \"education_num\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "]\n",
    "# A dictionary of the categorical features and their vocabulary.\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"workclass\": sorted(list(train_data[\"workclass\"].unique())),\n",
    "    \"education\": sorted(list(train_data[\"education\"].unique())),\n",
    "    \"marital_status\": sorted(list(train_data[\"marital_status\"].unique())),\n",
    "    \"occupation\": sorted(list(train_data[\"occupation\"].unique())),\n",
    "    \"relationship\": sorted(list(train_data[\"relationship\"].unique())),\n",
    "    \"race\": sorted(list(train_data[\"race\"].unique())),\n",
    "    \"gender\": sorted(list(train_data[\"gender\"].unique())),\n",
    "    \"native_country\": sorted(list(train_data[\"native_country\"].unique())),\n",
    "}\n",
    "# Name of the column to be used as instances weight.\n",
    "WEIGHT_COLUMN_NAME = \"fnlwgt\"\n",
    "# A list of the categorical feature names.\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
    "# A list of all the input features.\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "# A list of column default values for each feature.\n",
    "COLUMN_DEFAULTS = [\n",
    "    [0.0] if feature_name in NUMERIC_FEATURE_NAMES + [WEIGHT_COLUMN_NAME] else [\"NA\"]\n",
    "    for feature_name in CSV_HEADER\n",
    "]\n",
    "# The name of the target feature.\n",
    "TARGET_FEATURE_NAME = \"income_bracket\"\n",
    "# A list of the labels of the target features.\n",
    "TARGET_LABELS = [\" <=50K\", \" >50K\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11099806",
   "metadata": {},
   "source": [
    "##  Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1fb9b9",
   "metadata": {},
   "source": [
    "### The hyperparameters includes model architecture and training configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e596393",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "DROPOUT_RATE = 0.2\n",
    "BATCH_SIZE = 265\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "NUM_TRANSFORMER_BLOCKS = 3  # Number of transformer blocks.\n",
    "NUM_HEADS = 4  # Number of attention heads.\n",
    "EMBEDDING_DIMS = 16  # Embedding dimensions of the categorical features.\n",
    "MLP_HIDDEN_UNITS_FACTORS = [\n",
    "    2,\n",
    "    1,\n",
    "]  # MLP hidden layer units, as factors of the number of inputs.\n",
    "NUM_MLP_BLOCKS = 2  # Number of MLP blocks in the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c3cbc5",
   "metadata": {},
   "source": [
    "##  Implement data reading pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6124a5b9",
   "metadata": {},
   "source": [
    "### We define an input function that reads and parses the file, then converts features and labels into atf.data.Dataset for training or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8412ab34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Safouane Elh\\anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py:2446: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    }
   ],
   "source": [
    "target_label_lookup = layers.StringLookup(\n",
    "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
    ")\n",
    "\n",
    "\n",
    "def prepare_example(features, target):\n",
    "    target_index = target_label_lookup(target)\n",
    "    weights = features.pop(WEIGHT_COLUMN_NAME)\n",
    "    return features, target_index, weights\n",
    "\n",
    "\n",
    "def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_HEADER,\n",
    "        column_defaults=COLUMN_DEFAULTS,\n",
    "        label_name=TARGET_FEATURE_NAME,\n",
    "        num_epochs=1,\n",
    "        header=False,\n",
    "        na_value=\"?\",\n",
    "        shuffle=shuffle,\n",
    "    ).map(prepare_example, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    return dataset.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eb8818",
   "metadata": {},
   "source": [
    "##  Implement a training and evaluation procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6cadc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    model,\n",
    "    train_data_file,\n",
    "    test_data_file,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    weight_decay,\n",
    "    batch_size,\n",
    "):\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")],\n",
    "    )\n",
    "\n",
    "    train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)\n",
    "    validation_dataset = get_dataset_from_csv(test_data_file, batch_size)\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "    history = model.fit(\n",
    "        train_dataset, epochs=num_epochs, validation_data=validation_dataset\n",
    "    )\n",
    "    print(\"Model training finished\")\n",
    "\n",
    "    _, accuracy = model.evaluate(validation_dataset, verbose=0)\n",
    "\n",
    "    print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c8d473",
   "metadata": {},
   "source": [
    "##  Create model inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc65ce76",
   "metadata": {},
   "source": [
    "### Define the inputs for the models as a dictionary, where the key is the feature name, and the value is a keras.layers.Input tensor with the corresponding feature shape and data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30881c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.float32\n",
    "            )\n",
    "        else:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.string\n",
    "            )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca94ff7",
   "metadata": {},
   "source": [
    "##  Encode Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df410cc",
   "metadata": {},
   "source": [
    "###  We encode the categorical features as embeddings, using a fixed embedding_dims for all the features, regardless their vocabulary sizes. This is required for the Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e92cff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_inputs(inputs, embedding_dims):\n",
    "\n",
    "    encoded_categorical_feature_list = []\n",
    "    numerical_feature_list = []\n",
    "\n",
    "    for feature_name in inputs:\n",
    "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "\n",
    "            # Get the vocabulary of the categorical feature.\n",
    "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "\n",
    "            # Create a lookup to convert string values to an integer indices.\n",
    "            # Since we are not using a mask token nor expecting any out of vocabulary\n",
    "            # (oov) token, we set mask_token to None and  num_oov_indices to 0.\n",
    "            lookup = layers.StringLookup(\n",
    "                vocabulary=vocabulary,\n",
    "                mask_token=None,\n",
    "                num_oov_indices=0,\n",
    "                output_mode=\"int\",\n",
    "            )\n",
    "\n",
    "            # Convert the string input values into integer indices.\n",
    "            encoded_feature = lookup(inputs[feature_name])\n",
    "\n",
    "            # Create an embedding layer with the specified dimensions.\n",
    "            embedding = layers.Embedding(\n",
    "                input_dim=len(vocabulary), output_dim=embedding_dims\n",
    "            )\n",
    "\n",
    "            # Convert the index values to embedding representations.\n",
    "            encoded_categorical_feature = embedding(encoded_feature)\n",
    "            encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Use the numerical features as-is.\n",
    "            numerical_feature = tf.expand_dims(inputs[feature_name], -1)\n",
    "            numerical_feature_list.append(numerical_feature)\n",
    "\n",
    "    return encoded_categorical_feature_list, numerical_feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe1076b",
   "metadata": {},
   "source": [
    "##  Implement an MLP block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38298e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n",
    "\n",
    "    mlp_layers = []\n",
    "    for units in hidden_units:\n",
    "        mlp_layers.append(normalization_layer),\n",
    "        mlp_layers.append(layers.Dense(units, activation=activation))\n",
    "        mlp_layers.append(layers.Dropout(dropout_rate))\n",
    "\n",
    "    return keras.Sequential(mlp_layers, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14933e5",
   "metadata": {},
   "source": [
    "##  Experiment 1: a baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a919a97e",
   "metadata": {},
   "source": [
    "###  We create a simple multi-layer feed-forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70f82b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model weights: 109629\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "def create_baseline_model(\n",
    "    embedding_dims, num_mlp_blocks, mlp_hidden_units_factors, dropout_rate\n",
    "):\n",
    "\n",
    "    # Create model inputs.\n",
    "    inputs = create_model_inputs()\n",
    "    # encode features.\n",
    "    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "        inputs, embedding_dims\n",
    "    )\n",
    "    # Concatenate all features.\n",
    "    features = layers.concatenate(\n",
    "        encoded_categorical_feature_list + numerical_feature_list\n",
    "    )\n",
    "    # Compute Feedforward layer units.\n",
    "    feedforward_units = [features.shape[-1]]\n",
    "\n",
    "    # Create several feedforwad layers with skip connections.\n",
    "    for layer_idx in range(num_mlp_blocks):\n",
    "        features = create_mlp(\n",
    "            hidden_units=feedforward_units,\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=keras.activations.gelu,\n",
    "            normalization_layer=layers.LayerNormalization(epsilon=1e-6),\n",
    "            name=f\"feedforward_{layer_idx}\",\n",
    "        )(features)\n",
    "\n",
    "    # Compute MLP hidden_units.\n",
    "    mlp_hidden_units = [\n",
    "        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "    ]\n",
    "    # Create final MLP.\n",
    "    features = create_mlp(\n",
    "        hidden_units=mlp_hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=keras.activations.selu,\n",
    "        normalization_layer=layers.BatchNormalization(),\n",
    "        name=\"MLP\",\n",
    "    )(features)\n",
    "\n",
    "    # Add a sigmoid as a binary classifer.\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "baseline_model = create_baseline_model(\n",
    "    embedding_dims=EMBEDDING_DIMS,\n",
    "    num_mlp_blocks=NUM_MLP_BLOCKS,\n",
    "    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")\n",
    "\n",
    "print(\"Total model weights:\", baseline_model.count_params())\n",
    "keras.utils.plot_model(baseline_model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe47ccfe",
   "metadata": {},
   "source": [
    "### We train and evaluate the baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3af1e58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/15\n",
      "    121/Unknown - 10s 18ms/step - loss: 110623.6172 - accuracy: 0.7455WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 14s 52ms/step - loss: 110356.0000 - accuracy: 0.7460 - val_loss: 94824.4531 - val_accuracy: 0.7812\n",
      "Epoch 2/15\n",
      "122/123 [============================>.] - ETA: 0s - loss: 95354.1172 - accuracy: 0.7702WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 5s 40ms/step - loss: 95193.5391 - accuracy: 0.7700 - val_loss: 72918.4766 - val_accuracy: 0.8048\n",
      "Epoch 3/15\n",
      "122/123 [============================>.] - ETA: 0s - loss: 76648.0781 - accuracy: 0.7920WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 76551.5078 - accuracy: 0.7921 - val_loss: 68255.8984 - val_accuracy: 0.8160\n",
      "Epoch 4/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 72848.4141 - accuracy: 0.7999WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 3s 26ms/step - loss: 72848.4141 - accuracy: 0.7999 - val_loss: 67979.5078 - val_accuracy: 0.8114\n",
      "Epoch 5/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 70982.1641 - accuracy: 0.8054WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 4s 29ms/step - loss: 70982.1641 - accuracy: 0.8054 - val_loss: 67994.0938 - val_accuracy: 0.8061\n",
      "Epoch 6/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 69438.0000 - accuracy: 0.8096WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 5s 45ms/step - loss: 69438.0000 - accuracy: 0.8096 - val_loss: 69049.0234 - val_accuracy: 0.7991\n",
      "Epoch 7/15\n",
      "121/123 [============================>.] - ETA: 0s - loss: 69035.5859 - accuracy: 0.8106WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 68865.9453 - accuracy: 0.8108 - val_loss: 67734.9688 - val_accuracy: 0.8067\n",
      "Epoch 8/15\n",
      "121/123 [============================>.] - ETA: 0s - loss: 68423.3438 - accuracy: 0.8130WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 4s 36ms/step - loss: 68285.7578 - accuracy: 0.8132 - val_loss: 69364.3906 - val_accuracy: 0.7977\n",
      "Epoch 9/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 68305.4844 - accuracy: 0.8102WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 4s 34ms/step - loss: 68305.4844 - accuracy: 0.8102 - val_loss: 69492.9453 - val_accuracy: 0.7971\n",
      "Epoch 10/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 67729.1172 - accuracy: 0.8133WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 5s 42ms/step - loss: 67729.1172 - accuracy: 0.8133 - val_loss: 67118.9453 - val_accuracy: 0.8070\n",
      "Epoch 11/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 67099.3984 - accuracy: 0.8161WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 3s 27ms/step - loss: 67099.3984 - accuracy: 0.8161 - val_loss: 65844.3516 - val_accuracy: 0.8135\n",
      "Epoch 12/15\n",
      "122/123 [============================>.] - ETA: 0s - loss: 66959.4688 - accuracy: 0.8169WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 6s 47ms/step - loss: 66887.3203 - accuracy: 0.8171 - val_loss: 65008.1328 - val_accuracy: 0.8269\n",
      "Epoch 13/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 65620.3906 - accuracy: 0.8225WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 5s 40ms/step - loss: 65620.3906 - accuracy: 0.8225 - val_loss: 64426.9648 - val_accuracy: 0.8234\n",
      "Epoch 14/15\n",
      "122/123 [============================>.] - ETA: 0s - loss: 64384.0938 - accuracy: 0.8275WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 4s 29ms/step - loss: 64306.7461 - accuracy: 0.8276 - val_loss: 65215.0547 - val_accuracy: 0.8168\n",
      "Epoch 15/15\n",
      "122/123 [============================>.] - ETA: 0s - loss: 63192.6992 - accuracy: 0.8307WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 4s 33ms/step - loss: 63147.9141 - accuracy: 0.8307 - val_loss: 62258.0625 - val_accuracy: 0.8308\n",
      "Model training finished\n",
      "WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 83.08%\n"
     ]
    }
   ],
   "source": [
    "history = run_experiment(\n",
    "    model=baseline_model,\n",
    "    train_data_file=train_data_file,\n",
    "    test_data_file=test_data_file,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928fb1cb",
   "metadata": {},
   "source": [
    "##  Experiment 2: TabTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf16c50",
   "metadata": {},
   "source": [
    "### we create the model input, encoded all categorical features as embeddings and add the columns embeddding to categorical feature embeddings, concatenated the input numerical features with the categorical features into the final  transfomer block, with using the softmax classifier at the end of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "118a6fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model weights: 87479\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "def create_tabtransformer_classifier(\n",
    "    num_transformer_blocks,\n",
    "    num_heads,\n",
    "    embedding_dims,\n",
    "    mlp_hidden_units_factors,\n",
    "    dropout_rate,\n",
    "    use_column_embedding=False,\n",
    "):\n",
    "\n",
    "    # Create model inputs.\n",
    "    inputs = create_model_inputs()\n",
    "    # encode features.\n",
    "    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "        inputs, embedding_dims\n",
    "    )\n",
    "    # Stack categorical feature embeddings for the Tansformer.\n",
    "    encoded_categorical_features = tf.stack(encoded_categorical_feature_list, axis=1)\n",
    "    # Concatenate numerical features.\n",
    "    numerical_features = layers.concatenate(numerical_feature_list)\n",
    "\n",
    "    # Add column embedding to categorical feature embeddings.\n",
    "    if use_column_embedding:\n",
    "        num_columns = encoded_categorical_features.shape[1]\n",
    "        column_embedding = layers.Embedding(\n",
    "            input_dim=num_columns, output_dim=embedding_dims\n",
    "        )\n",
    "        column_indices = tf.range(start=0, limit=num_columns, delta=1)\n",
    "        encoded_categorical_features = encoded_categorical_features + column_embedding(\n",
    "            column_indices\n",
    "        )\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for block_idx in range(num_transformer_blocks):\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dims,\n",
    "            dropout=dropout_rate,\n",
    "            name=f\"multihead_attention_{block_idx}\",\n",
    "        )(encoded_categorical_features, encoded_categorical_features)\n",
    "        # Skip connection 1.\n",
    "        x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n",
    "            [attention_output, encoded_categorical_features]\n",
    "        )\n",
    "        # Layer normalization 1.\n",
    "        x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n",
    "        # Feedforward.\n",
    "        feedforward_output = create_mlp(\n",
    "            hidden_units=[embedding_dims],\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=keras.activations.gelu,\n",
    "            normalization_layer=layers.LayerNormalization(epsilon=1e-6),\n",
    "            name=f\"feedforward_{block_idx}\",\n",
    "        )(x)\n",
    "        # Skip connection 2.\n",
    "        x = layers.Add(name=f\"skip_connection2_{block_idx}\")([feedforward_output, x])\n",
    "        # Layer normalization 2.\n",
    "        encoded_categorical_features = layers.LayerNormalization(\n",
    "            name=f\"layer_norm2_{block_idx}\", epsilon=1e-6\n",
    "        )(x)\n",
    "\n",
    "    # Flatten the \"contextualized\" embeddings of the categorical features.\n",
    "    categorical_features = layers.Flatten()(encoded_categorical_features)\n",
    "    # Apply layer normalization to the numerical features.\n",
    "    numerical_features = layers.LayerNormalization(epsilon=1e-6)(numerical_features)\n",
    "    # Prepare the input for the final MLP block.\n",
    "    features = layers.concatenate([categorical_features, numerical_features])\n",
    "\n",
    "    # Compute MLP hidden_units.\n",
    "    mlp_hidden_units = [\n",
    "        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "    ]\n",
    "    # Create final MLP.\n",
    "    features = create_mlp(\n",
    "        hidden_units=mlp_hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=keras.activations.selu,\n",
    "        normalization_layer=layers.BatchNormalization(),\n",
    "        name=\"MLP\",\n",
    "    )(features)\n",
    "\n",
    "    # Add a sigmoid as a binary classifer.\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "tabtransformer_model = create_tabtransformer_classifier(\n",
    "    num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embedding_dims=EMBEDDING_DIMS,\n",
    "    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")\n",
    "\n",
    "print(\"Total model weights:\", tabtransformer_model.count_params())\n",
    "keras.utils.plot_model(tabtransformer_model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c172f16",
   "metadata": {},
   "source": [
    "### We train and evaluate the TabTransformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c02ed1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/15\n",
      "    123/Unknown - 25s 76ms/step - loss: 80281.6016 - accuracy: 0.7991WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 29s 108ms/step - loss: 80281.6016 - accuracy: 0.7991 - val_loss: 69474.3203 - val_accuracy: 0.8274\n",
      "Epoch 2/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 69593.2578 - accuracy: 0.8271WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 13s 102ms/step - loss: 69593.2578 - accuracy: 0.8271 - val_loss: 64887.3984 - val_accuracy: 0.8338\n",
      "Epoch 3/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 66330.0469 - accuracy: 0.8324WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 12s 101ms/step - loss: 66330.0469 - accuracy: 0.8324 - val_loss: 61989.9102 - val_accuracy: 0.8451\n",
      "Epoch 4/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 64954.6328 - accuracy: 0.8362WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 12s 102ms/step - loss: 64954.6328 - accuracy: 0.8362 - val_loss: 61848.6797 - val_accuracy: 0.8447\n",
      "Epoch 5/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 63689.2930 - accuracy: 0.8383WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 11s 91ms/step - loss: 63689.2930 - accuracy: 0.8383 - val_loss: 64707.2109 - val_accuracy: 0.8369\n",
      "Epoch 6/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 63169.1367 - accuracy: 0.8399WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 14s 116ms/step - loss: 63169.1367 - accuracy: 0.8399 - val_loss: 64265.4180 - val_accuracy: 0.8380\n",
      "Epoch 7/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 62808.8164 - accuracy: 0.8410WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 13s 106ms/step - loss: 62808.8164 - accuracy: 0.8410 - val_loss: 61941.3203 - val_accuracy: 0.8444\n",
      "Epoch 8/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 62341.7578 - accuracy: 0.8419WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 14s 116ms/step - loss: 62341.7578 - accuracy: 0.8419 - val_loss: 61300.9531 - val_accuracy: 0.8450\n",
      "Epoch 9/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 62056.3867 - accuracy: 0.8416WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 13s 103ms/step - loss: 62056.3867 - accuracy: 0.8416 - val_loss: 61084.3398 - val_accuracy: 0.8465\n",
      "Epoch 10/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 61800.3594 - accuracy: 0.8435WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 10s 80ms/step - loss: 61800.3594 - accuracy: 0.8435 - val_loss: 61235.9922 - val_accuracy: 0.8452\n",
      "Epoch 11/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 61527.7188 - accuracy: 0.8430WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 12s 94ms/step - loss: 61527.7188 - accuracy: 0.8430 - val_loss: 61312.9219 - val_accuracy: 0.8453\n",
      "Epoch 12/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 61432.2852 - accuracy: 0.8442WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 11s 93ms/step - loss: 61432.2852 - accuracy: 0.8442 - val_loss: 61121.5195 - val_accuracy: 0.8455\n",
      "Epoch 13/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 61302.0000 - accuracy: 0.8436WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 9s 73ms/step - loss: 61302.0000 - accuracy: 0.8436 - val_loss: 61409.8047 - val_accuracy: 0.8452\n",
      "Epoch 14/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 61045.6406 - accuracy: 0.8448WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 10s 81ms/step - loss: 61045.6406 - accuracy: 0.8448 - val_loss: 61027.0508 - val_accuracy: 0.8457\n",
      "Epoch 15/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 60898.4727 - accuracy: 0.8449WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 13s 104ms/step - loss: 60898.4727 - accuracy: 0.8449 - val_loss: 61069.3828 - val_accuracy: 0.8437\n",
      "Model training finished\n",
      "WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 84.37%\n"
     ]
    }
   ],
   "source": [
    "history = run_experiment(\n",
    "    model=tabtransformer_model,\n",
    "    train_data_file=train_data_file,\n",
    "    test_data_file=test_data_file,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5a78a5",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "For the first step in this lab i learn how i can load our dataset into a pandas dataframe and how we can splited into a training and validation with storing in separate CSV files.\n",
    "and in the second i discover how we can define the metadata of the dataset for reading and parsing the data into input features and encondinf each input feature to appropriate type. after i see how we can configure the parameters for the model. and define an input function that read and parses the file,\n",
    "for the 3rd step i see how we can implement a procedure and how we can define the optimizer technique, the parameters (learning rate, weight decay), the loss function and the metrics.\n",
    "at the 4th step i find out how we can define the input features for the model as a dictionary. and how we can use the encode inputs methods also how ce can encode the categorical features as embeddings\n",
    "at the final i pick up what we can do to implement an MLP block. and see the diffrence between the simple multi-layer feed-forward network and The TabTransformer architecture works, and i see that the TabTransformer significantly outperforms of the MLP\n",
    "â€‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979d7b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
